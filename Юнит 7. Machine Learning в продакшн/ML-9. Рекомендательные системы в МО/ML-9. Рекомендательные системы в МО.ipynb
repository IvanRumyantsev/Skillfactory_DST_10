{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('base': conda)",
   "display_name": "Python 3.7.9 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a8f61be024eba58adef938c9aa1e29e02cb3dece83a5348b1a2dafd16a070453"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 9.1. О чем этот модуль\n",
    "В этом модуле мы рассмотрим классические методы рекомендательных систем:\n",
    "\n",
    "- Ассоциативные правила.\n",
    "- Коллаборативная фильтрация.\n",
    "- Алгоритмы SVD и ALS."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 9.2. Характеристики рекомендательных систем\n",
    "## Задача рекомендательной системы\n",
    "\n",
    "Проинформировать пользователя о продукте, который ему может быть наиболее интересен в данный момент времени.\n",
    "\n",
    "## КОМУ НУЖНЫ РЕКОМЕНДАТЕЛЬНЫЕ СИСТЕМЫ?\n",
    "\n",
    "→ клиенту — получает информацию о наиболее подходящих для него товарах, интересных мультимедийных продуктах и т. д. \n",
    "\n",
    "→ сервису — зарабатывает на предоставлении услуг.\n",
    "\n",
    "## СТЕПЕНЬ ПЕРСОНАЛИЗАЦИИ\n",
    "\n",
    "→ Неперсональные рекомендации — когда вам рекомендуют то же самое, что всем остальным. \n",
    "\n",
    "→ Персональные рекомендации используют всю доступную информацию о клиенте.\n",
    "\n",
    "→ Более продвинутый вариант — рекомендации на данных из текущей сессии. Вы посмотрели несколько товаров, и внизу страницы вам предлагаются похожие."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 9.3. Ассоциативные правила\n",
    "\n",
    "Для того чтобы сравнить ассоциативные правила по их силе (значимости) необходимо ввести несколько метрик — Support, Confidence и Lift."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   tr  cola  beer  diapers\n",
       "0   1     1     1        1\n",
       "1   2     0     0        0\n",
       "2   3     1     1        0\n",
       "3   4     1     1        1\n",
       "4   5     0     1        1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tr</th>\n      <th>cola</th>\n      <th>beer</th>\n      <th>diapers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "a = np.array([[1,1,1,1],[2, 0, 0, 0],[3,1,1,0],[4,1,1,1],[5,0,1,1]])\n",
    "df = pd.DataFrame(a, columns = ['tr', 'cola', 'beer', 'diapers'])\n",
    "df"
   ]
  },
  {
   "source": [
    "## SUPPORT\n",
    "$$sup(x) = \\frac{ \\{ t \\in T; X \\in t \\} }{|T|}$$\n",
    "\n",
    "Здесь  — это X itemset, в котором находится items, T — количество транзакций.\n",
    "\n",
    "Можно охарактеризовать этот показатель как индекс частоты встречаемости конкретного продукта в имеющихся транзакциях. Это для того случая, если нам интересен один конкретный продукт (item).\n",
    "\n",
    "Чаще нам бывает важно, насколько часто какие-то два продукта встречаются вместе. Для такого случая мы будем рассчитывать следующий вариант показателя:\n",
    "\n",
    "$$sup(x_1 \\cup x_2) = \\frac{ \\sigma (x_1 \\cup x_2) }{|T|}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Предположим, что  есть несколько транзакций, в которых присутствует пиво, подгузники и кола. Мы считаем количество совместных транзакций, в которых есть пиво и подгузники, и делим на общее количество транзакций.\n",
    "\n",
    "        Получается, Support такого правила равен 3/5, или 60 %. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   beer  diapers\n",
       "0     1        1\n",
       "3     1        1\n",
       "4     1        1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>beer</th>\n      <th>diapers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "df[(df.beer == 1)&(df.diapers == 1)][['beer', 'diapers']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "supp_b_and_d = df[(df.beer == 1)&(df.diapers == 1)].shape[0]/df.shape[0]\n",
    "supp_b_and_d"
   ]
  },
  {
   "source": [
    "## CONFIDENCE\n",
    "\n",
    "Этот показатель высчитывается на основе метрики Support.\n",
    "\n",
    "$$conf(x_1 \\cup x_2) = \\frac{ supp (x_1 \\cup x_2) }{sup(T)}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Он определяет, как часто правило срабатывает для всего датасета.\n",
    "\n",
    "У нас есть Support пива и подгузников, которое мы посчитали. Также мы можем посчитать Support только пива. \n",
    "        \n",
    "        Отношение этих двух Support будет равно 3/4, или 75 %."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "supp_b = df[(df.beer == 1)].shape[0]/df.shape[0]\n",
    "supp_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "np.around(supp_b_and_d/supp_b, 2)"
   ]
  },
  {
   "source": [
    "## LIFT \n",
    "\n",
    "Ещё одна метрика — Lift. Она вычисляется следующим образом:\n",
    "\n",
    "- вычисляется Support совместной встречаемости двух продуктов;\n",
    "- делится на произведение Support каждого из этих продуктов.\n",
    "\n",
    "$$ lift(x_1 \\cup x_2) = \\frac{ supp (x_1 \\cup x_2) }{supp(x_1)*supp(x_2)} $$\n",
    "\n",
    "\n",
    "$$ lift = \\frac{Confidence}{Expectedconfidence} = \\frac{Подгузники|Пиво}{Подгузники} $$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "supp_d = df[(df.diapers == 1)].shape[0]/df.shape[0]\n",
    "supp_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.25"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "np.around(supp_b_and_d/(supp_d*supp_b), 2)"
   ]
  },
  {
   "source": [
    "## АЛГОРИТМ APRIORI\n",
    "\n",
    "Рассмотрим один из алгоритмов для построения ассоциативных правил — Apriori.\n",
    "\n",
    "Apriori использует следующее утверждение: \n",
    "\n",
    "если X⊆Y, то supp(X) ≥ supp(Y).\n",
    "\n",
    "Отсюда следуют два свойства:\n",
    "\n",
    "- если Y встречается часто, то любое подмножество X: X⊆Y также встречается часто\n",
    "- если X встречается редко, то любое супермножество Y: Y ⊇X также встречается редко\n",
    "\n",
    "Apriori по уровням проходит по префиксному дереву и рассчитывает частоту X встречаемости подмножеств  в D. \n",
    "\n",
    "Таким образом:\n",
    "\n",
    "- исключаются редкие подмножества и все их супермножества.\n",
    "- рассчитывается supp(X) для каждого подходящего кандидата X размера k на уровне k."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Задание 9.3.1\n",
    "\n",
    "1. Какой показатель определяет частоту срабатываемости правила на нашем датасете?\n",
    "- support\n",
    "- confidence верно\n",
    "- lift\n",
    "\n",
    "2. Предположим, у нас есть данные о 300 транзакциях. Мы отметили, что в 62 из них покупают вместе форель и соевый соус. Рассчитайте показатель support, округлите до сотых (пример: 0.99).\n",
    "- 0.21  верно \n",
    "\n",
    "3. Какие данные наиболее удобны для работы с ассоциативными правилами?\n",
    "- Порядковые\n",
    "- Бинарные верно\n",
    "- Количественные\n",
    "- Номинальные"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 9.4. Практика"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data_fin.csv', sep=';')"
   ]
  },
  {
   "source": [
    "Здесь есть ID для пользователя, ID для фильма и рейтинг, который поставил данный пользователь данному фильму."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 100480507 entries, 0 to 100480506\nData columns (total 3 columns):\n #   Column    Dtype  \n---  ------    -----  \n 0   Cust_Id   int64  \n 1   Rating    float64\n 2   Movie_Id  int64  \ndtypes: float64(1), int64(2)\nmemory usage: 2.2 GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "source": [
    "Из полученного датасета возьмем только те записи, у которых наивысший рейтинг (5) и объединим их по \"Cust_Id\". Фильмы сгруппируем в строчку с разделителем \"пробел\" так, чтобы для каждого пользователя была строка с Id тех фильмов, которые ему понравились:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = df[df['Rating']==5].groupby('Cust_Id')['Movie_Id'].apply(lambda r: ' '.join([str(A) for A in r]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Cust_Id\n",
       "6     175 457 886 1467 2372 2452 2782 3290 4043 4633...\n",
       "7     8 30 83 175 257 283 285 313 357 457 458 468 50...\n",
       "8     1202 1799 1905 2186 3610 3925 4306 5054 5317 5...\n",
       "10    473 985 1542 1905 2172 3124 3371 3962 4043 430...\n",
       "25                4432 6786 7605 9326 10643 15107 15270\n",
       "Name: Movie_Id, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "good.head()"
   ]
  },
  {
   "source": [
    "Сначала идёт ID пользователя, дальше через пробел фильмы, которые нравятся этому пользователю."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apyori"
   ]
  },
  {
   "source": [
    "Cделаем несколько ассоциативных правил. Мы можем регулировать их количество, меняя параметры алгоритмов. Посмотрим, какие ассоциативные правила получаются для support = 0.1."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_rules = apyori.apriori(good.apply(lambda r: r.split(' ')), \n",
    "                                   min_support=0.04, \n",
    "                                   min_confidence=0.1, min_lift=2, \n",
    "                                   min_length=2)"
   ]
  },
  {
   "source": [
    "Строго говоря, мы получили не сами ассоциативные правила, а генератор. Это можно проверить, если, например, вызвать переменную association_rules:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<generator object apriori at 0x000001B7D747FDC8>"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_df = pd.DataFrame(columns = ['from', 'to', 'confidence', 'support', 'lift'])\n",
    "for item in association_rules:\n",
    "    pair = item[0] \n",
    "    items = [x for x in pair]\n",
    "    asr_df.loc[len(asr_df), :] =  ' '.join(list(item[2][0][0])), \\\n",
    "                                  ' '.join(list(item[2][0][1])),\\\n",
    "                                  item[2][0][2], item[1], item[2][0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       from                     to confidence    support     lift\n",
       "1718  11521  2452 5582 14961 14240   0.259398  0.0540124  4.59819\n",
       "583   10042             11283 2782   0.260829   0.040732   3.1777\n",
       "466   17157                   7230   0.314779  0.0447288  2.53424\n",
       "611   10042             2452 14240   0.470967  0.0735479  3.03236\n",
       "79    11064                   6974   0.414779  0.0627631  3.23419\n",
       "534    3938                   3962   0.511215  0.0716239  2.98322\n",
       "1869  14240    7057 2782 2452 7230   0.206185  0.0412734  4.77095\n",
       "1178   2782              5582 9628   0.285592  0.0438315  2.90674\n",
       "607   10042            14961 14240   0.368508  0.0575476   3.4082\n",
       "1260  10042        16954 2452 7230   0.266298  0.0415861   5.2994"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>from</th>\n      <th>to</th>\n      <th>confidence</th>\n      <th>support</th>\n      <th>lift</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1718</th>\n      <td>11521</td>\n      <td>2452 5582 14961 14240</td>\n      <td>0.259398</td>\n      <td>0.0540124</td>\n      <td>4.59819</td>\n    </tr>\n    <tr>\n      <th>583</th>\n      <td>10042</td>\n      <td>11283 2782</td>\n      <td>0.260829</td>\n      <td>0.040732</td>\n      <td>3.1777</td>\n    </tr>\n    <tr>\n      <th>466</th>\n      <td>17157</td>\n      <td>7230</td>\n      <td>0.314779</td>\n      <td>0.0447288</td>\n      <td>2.53424</td>\n    </tr>\n    <tr>\n      <th>611</th>\n      <td>10042</td>\n      <td>2452 14240</td>\n      <td>0.470967</td>\n      <td>0.0735479</td>\n      <td>3.03236</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>11064</td>\n      <td>6974</td>\n      <td>0.414779</td>\n      <td>0.0627631</td>\n      <td>3.23419</td>\n    </tr>\n    <tr>\n      <th>534</th>\n      <td>3938</td>\n      <td>3962</td>\n      <td>0.511215</td>\n      <td>0.0716239</td>\n      <td>2.98322</td>\n    </tr>\n    <tr>\n      <th>1869</th>\n      <td>14240</td>\n      <td>7057 2782 2452 7230</td>\n      <td>0.206185</td>\n      <td>0.0412734</td>\n      <td>4.77095</td>\n    </tr>\n    <tr>\n      <th>1178</th>\n      <td>2782</td>\n      <td>5582 9628</td>\n      <td>0.285592</td>\n      <td>0.0438315</td>\n      <td>2.90674</td>\n    </tr>\n    <tr>\n      <th>607</th>\n      <td>10042</td>\n      <td>14961 14240</td>\n      <td>0.368508</td>\n      <td>0.0575476</td>\n      <td>3.4082</td>\n    </tr>\n    <tr>\n      <th>1260</th>\n      <td>10042</td>\n      <td>16954 2452 7230</td>\n      <td>0.266298</td>\n      <td>0.0415861</td>\n      <td>5.2994</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "asr_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2023"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "len(asr_df)"
   ]
  },
  {
   "source": [
    "Мы видим здесь таблицу, где 2023 ассоциативных правила, и для каждого рассчитаны известные нам показатели.\n",
    "\n",
    "Для того чтобы перейти от Id фильмов, к их названиям, нужно загрузить еще один файл, в котором содержится Id фильма, год его производства и название:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = pd.read_csv('movie_titles.csv', encoding = \"ISO-8859-1\", \n",
    "                     header = None, \n",
    "                     names = ['Movie_Id', 'Year', 'Name'])"
   ]
  },
  {
   "source": [
    "Мы можем написать процедуру, которая будет выводить названия фильмов в ассоциативном правиле и фильм, которое это ассоциативное правило рекомендует:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rule_title(rule):\n",
    "    print(titles[titles.Movie_Id.isin(rule['from'].split(' '))]['Name'].values)\n",
    "    print('----------->')\n",
    "    print(titles[titles.Movie_Id == int(rule['to'])]['Name'].values)"
   ]
  },
  {
   "source": [
    "Можем посмотреть, как выглядит это правило. Например, вызовем сотое правило:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Monsters']\n----------->\n['Finding Nemo (Widescreen)']\n"
     ]
    }
   ],
   "source": [
    "get_rule_title(asr_df.loc[99])"
   ]
  },
  {
   "source": [
    "То есть, если человеку нравится фильм «Монстры», то мы советуем ему посмотреть фильм «В поисках Немо».\n",
    "\n",
    "Перейдём к построению рекомендаций для случайного человека под id=159992. Посмотрим, какие фильмы он смотрел и как он их оценил. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1144     The Wedding Planner\n",
       "2151         What Women Want\n",
       "5316       Miss Congeniality\n",
       "6286            Pretty Woman\n",
       "6385              Sister Act\n",
       "7233            Men of Honor\n",
       "10358          Runaway Bride\n",
       "11148      Maid in Manhattan\n",
       "13049       Two Weeks Notice\n",
       "15581     Sweet Home Alabama\n",
       "Name: Name, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "j = 159992\n",
    "titles[titles.Movie_Id.isin(good.iloc[j].split(' '))]['Name']"
   ]
  },
  {
   "source": [
    "Как мы можем посчитать посчитать рекомендации для этого человека? Мы можем пройтись по всем правилам в нашей таблице и проверить, если они присутствуют в просмотрах человека и у них высокий рейтинг, значит это правило ему подходит и мы можем добавить этот фильм в список рекомендаций."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'Pretty Woman', 'Dirty Dancing'}\n"
     ]
    }
   ],
   "source": [
    "def print_rule_title(rule):\n",
    "    return (titles[titles.Movie_Id == int(rule['to'])]['Name'].values)\n",
    "    \n",
    "\n",
    "result = []\n",
    "for A in asr_df.index:\n",
    "    if len(set(good.iloc[j].split(' ')) & set(asr_df['from'].loc[A].split(' '))) == len(asr_df['from'].loc[A].split(' ')):\n",
    "        result.append(print_rule_title(asr_df.loc[A])[0])\n",
    "print(set(result))"
   ]
  },
  {
   "source": [
    "Сложность такой рекомендательной системы состоит в том, что мы ограничены теми ассоциативными правилами, которые были созданы. Если фильм редкий, то он в эти ассоциативные правила, скорее всего, не попадёт.\n",
    "\n",
    "Также для разных людей будет разное количество рекомендаций, в зависимости от того, сколько фильмов посмотрел каждый из них. И, если нам захочется порекомендовать каждому пользователю минимум три фильма, то не с каждым пользователем мы сможем это сделать.\n",
    "\n",
    "Эту проблему можно решить с помощью алгоритма Коллаборативная фильтрация. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Задание 9.4.1\n",
    "\n",
    "1. Найдите фильмы, которые понравились пользователю с ID, равным 130. Понравившимися пользователю фильмами мы будем считать те фильмы, которым он поставил наивысшую оценку (5). Скопируйте все ID фильмов. Например: 68 943 325 1234."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['1865 3456 3962 5515 6029 6428 8159 8327 8782 8784 11165 11242 11701 12084 12232 14482'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "good[good.index == 130].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['The Patriot']\n----------->\n['The Green Mile']\n"
     ]
    }
   ],
   "source": [
    "get_rule_title(asr_df.loc[315])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 21\n",
    "result = []\n",
    "for A in asr_df.index:\n",
    "    if len(set(good.iloc[j].split(' ')) & set(asr_df['from'].loc[A].split(' '))) == len(asr_df['from'].loc[A].split(' ')):\n",
    "        result.append(print_rule_title(asr_df.loc[A])[0])\n",
    "result_df = pd.DataFrame(result, columns = ['film'])\n",
    "result_df['len'] = result_df.film.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The Sixth Sense'"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "result_df[result_df.len == result_df.len.min()]['film'].iloc[0]"
   ]
  },
  {
   "source": [
    "# 9.5. Коллаборативная фильтрация\n",
    "\n",
    "## В понимании этого материала мне помогли следующие ресурсы:\n",
    "\n",
    "- [Коллаборативная фильтрация - К.В. Воронцов](https://youtu.be/kfhqzkcfMqI 'youtube')"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## МАТРИЦА ПРЕДПОЧТЕНИЙ\n",
    "\n",
    "Расположим в матрице клиентов по строкам, а продукты — по столбцам. На пересечении строк и столбцов разметим оценку клиентов на эти продукты.  То есть первый клиент поставил второму товару 3, а третий клиент поставил первому товару 2 и так далее"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## КЛАСТЕРИЗАЦИЯ ПОЛЬЗОВАТЕЛЕЙ\n",
    "\n",
    "Далее мы можем применить алгоритм кластеризации для того, чтобы объединить людей.\n",
    "Выберем условную меру схожести пользователей по их истории оценок:\n",
    "\n",
    "$$sim(u, \\upsilon)$$\n",
    "\n",
    "Объединим пользователей в группы (кластеры) \n",
    "\n",
    "Разбиваем их на кластеры так, чтобы похожие пользователи оказались в одном кластере, а непохожие — в разных.\n",
    "\n",
    "Оценку пользователя объекту будем предсказывать как среднюю оценку кластера этому объекту.\n",
    "\n",
    "Итак, у нас есть пользователь, и у нас спрашивают, как он оценил данный фильм. Мы смотрим на кластер пользователя. Смотрим оценки пользователей из этого кластера на этот фильм (оценки тех пользователей, которые смотрели этот фильм и поставили оценку). Берем их и усредняем. Это и есть предсказание оценки фильма для нашего пользователя.\n",
    "\n",
    "### Проблемы алгоритма:\n",
    "\n",
    "- Нечего рекомендовать новым/нетипичным пользователям. Если у нас появляется пользователь, который не похож ни на кого, то мы не знаем, в какой кластер его отнести. Конечно, мы первоначально относим его в какой-то случайный кластер, но рекомендации в таком случае будут плохие.\n",
    "\n",
    "- Не учитывается специфика каждого пользователя. По сути, мы выявляем некоторые шаблонные паттерны поведения и предпочтений, и для каждого паттерна выделяем свои рекомендации. Но на самом деле, все пользователи немного отличаются друг от друга даже в одном кластере, поэтому возникают неточности.\n",
    "\n",
    "- Если в кластере никто не оценивал объект, то предсказание сделать не получится."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## USER-BASED\n",
    "\n",
    "Для того, чтобы преодолеть эти сложности, можно обратиться к другому алгоритму, который позволяет уйти от решения задачи кластеризации — User-based.\n",
    "\n",
    "В этом алгоритме мы заменяем жесткую кластеризацию на следующую формулу:\n",
    "\n",
    "$$ \\hat{r_{ui}} = \\bar{r_u}+\\frac{\\sum_{v \\in U_i} {sim(u, v)(r_{ui}-\\bar{r_v})}}{\\sum_{v \\in U_i} sim(u,v)} $$\n",
    "\n",
    "Разберемся с обозначениями, которые используются в этой формуле:\n",
    "\n",
    "$\\bar{r_u}$ — средняя оценка пользователя $u$\n",
    "\n",
    "$\\bar{r_v}$— средняя оценка пользователя $v$\n",
    "\n",
    "Оценка пользователя $\\hat{r_{ui}}$, которую мы предсказываем для него, состоит из двух частей:\n",
    "\n",
    "- Непосредственно его средняя оценка.\n",
    "- Слагаемое, состоящее из: $r_{ui} - \\bar{r_v}$  — разница в оценках с другими пользователями, т. е. похожесть пользователей. Эта разница домножается на похожесть пользователей. То есть в числителе оказалась средневзвешенная разница в оценках. А в знаменателе находится сумма показателей схожести.\n",
    "\n",
    "По каждому клиенту подбираем релевантный для него товар в рамках группы клиентов, но не решаем задачу кластеризации, а усредняем интересы данной группы в дистанции нескольких соседей.\n",
    "\n",
    "Здесь мы руководствуемся по сути идеей, что видео можно порекомендовать человеку, если оно понравится его друзьям."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## ITEM-BASED\n",
    "\n",
    "Однако, если мы транспонируем матрицу предпочтений и будем решать ту же самую задачу не в рамках клиентов, а в рамках items, то мы получим более устойчивое решение.\n",
    "\n",
    "$$ \\hat{r_{ui}} = \\bar{r_i}+\\frac{\\sum_{j \\in I_u} {sim(i,j)(r_{uj}-\\bar{r_j})}}{\\sum_{j \\in I_u} sim(i,j)} $$\n",
    "\n",
    "По формуле (которая очень похожа на формулу из предыдущего подхода) можно понять, что этот подход симметричен и использует ту же самую идею. Только теперь у нас не пользователи похожи, а объекты (items) похожи.\n",
    "\n",
    "То есть, если мы говорим о рекомендации фильмов, то мы теперь рекомендуем пользователю фильм, который похож на те фильмы, которые уже понравились этому пользователю ранее.\n",
    "\n",
    "Кроме того, у нас будет больше размерность на каждый вектор items, чем размерность вектора клиентов по items. За счёт этого будет больше оценок, выше статистическая значимость и модель будет более устойчива к переобучению. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Преимущества\n",
    "\n",
    "- Меньше размерность матрицы расстояний\n",
    "- Легче вычислять\n",
    "- Модель более устойчива к переобучению\n",
    "- Можно реже обновлять\n",
    "- Меньше подвержены изменению предпочтений со временем\n",
    "\n",
    "### Недостатки\n",
    "\n",
    "- Проблема холодного старта\n",
    "- Плохие предсказания для новых/нетипичных пользователей/объектов\n",
    "- Тривиальность рекомендаций\n",
    "- Ресурсоемкость вычислений"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 9.6. Практика\n",
    "\n",
    "Продолжим работать с датасетом Netflix.\n",
    "\n",
    "Возьмём подвыборку из 10000 случайных кастомеров и 5000 фильмов. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import surprise\n",
    "from surprise import Reader, Dataset\n",
    "from surprise import KNNBasic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_fin.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_sample = df.Cust_Id.sample(300)\n",
    "movie_sample = df.Movie_Id.sample(10000)"
   ]
  },
  {
   "source": [
    "Для генерации простых рекомендаций с помощью коллаборативной фильтрации можно воспользоваться модулем surprise. Загрузим в модуль surprise наш датасет с помощью метода Reader."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Возьмем только те оценки, которые относятся к выбранному подмножеству кастомеров и только те оценки, которые относятся к выбранному подмножеству фильмов. Именно в такой последовательности — сначала Cust_Id, затем Movie_Id, затем Rating."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "data = Dataset.load_from_df(df[df.Cust_Id.isin(cust_sample) &\n",
    "                              df.Movie_Id.isin(movie_sample)][['Cust_Id', 'Movie_Id', 'Rating']], reader)"
   ]
  },
  {
   "source": [
    "В модуле surprise есть несколько реализаций коллаборативной фильтрации. Мы возьмем одну из самых самых простых — принцип ближайших соседей."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Принцип коллаборативной фильтрации заключается в следующем:\n",
    "\n",
    "Для каждого человека находится небольшое множество похожих на него зрителей с оценками примерно такими же, какие поставил человек на ряд фильмов (item). Из этой группы можно усреднить оценки на просмотренные фильмы и для тех членов группы, у которых ещё не было просмотров этих фильмов экстраполировать значения оценок в этих ячейках.\n",
    "\n",
    "Таким образом, у нас появляется некая средняя оценка в группе для каждого фильма из просмотренных, и мы можем предположить, что тем людям, которые ещё не успели посмотреть эти фильмы, они понравятся."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Так как размерность по пользователям больше, чем размерность по фильмам, то выгоднее использовать не user-based алгоритм, а item-based. В этом случае вектор будет состоять не из оценок одного пользователя на различные фильмы, а будет содержать все оценки фильма от многих пользователей. Таким образом мы получим больший вектор, но само количество векторов будет меньше. А если меньше количество векторов, то проще посчитать матрицу из взаимной дистанции.\n",
    "\n",
    "Именно это мы задаем в качестве параметров алгоритма:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {\n",
    "    'name': 'cosine',\n",
    "    'user_based': False\n",
    "}\n",
    " \n",
    "knn = KNNBasic(sim_options=sim_options)\n",
    "trainingSet = data.build_full_trainset()"
   ]
  },
  {
   "source": [
    "Запускаем алгоритм и формирует датасет для тренировки специальной функцией build_full_trainset().\n",
    "\n",
    "После этого проводим тренировку модели на сформированном тренировочном датасете:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Computing the cosine similarity matrix...\nDone computing similarity matrix.\n--- 18.98 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "knn.fit(trainingSet)\n",
    "print(\"--- %s seconds ---\" % np.around(time.time() - start_time, 3))"
   ]
  },
  {
   "source": [
    "С помощью натренированной модели мы можем проскорить остальные оценки. Для этого сгенерируем тестовый сет и построим предсказание по этому датасету:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "start_time = time.time()\n",
    "\n",
    "testSet = trainingSet.build_anti_testset()\n",
    "predictions = knn.test(testSet)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- 320.2757239341736 seconds ---\n"
     ]
    }
   ]
  },
  {
   "source": [
    "Результат получился не удобочитаемым. Поэтому давайте сделаем вспомогательную функцию, которая будет брать топ-3 фильмов и их оценки:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from collections import defaultdict\n",
    " \n",
    "def get_top3_recommendations(predictions, topN = 3):\n",
    "     \n",
    "    top_recs = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_recs[uid].append((iid, est))\n",
    "     \n",
    "    for uid, user_ratings in top_recs.items():\n",
    "        user_ratings.sort(key = lambda x: x[1], reverse = True)\n",
    "        top_recs[uid] = user_ratings[:topN]\n",
    "     \n",
    "    return top_recs"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 66,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Обрабатываем наше предсказание:\n",
    "top3_recommendations = get_top3_recommendations(predictions)"
   ]
  },
  {
   "source": [
    "С помощью следующей функции переведем тексты фильмов в удобочитаемый вид, то есть раскодируем заглавия фильмов. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import numpy as np\n",
    "def print_recs(i):\n",
    "    for (a, b) in top3_recommendations[i]:\n",
    "        print(titles[titles.Movie_Id == a]['Name'].values[0], np.round(b,2))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 68,
   "outputs": []
  },
  {
   "source": [
    "С помощью этой функции выведем рекомендации для случайного пользователя (предварительно загрузим movie_titles.csv):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = pd.read_csv('movie_titles.csv', encoding = \"ISO-8859-1\", \n",
    "                     header = None, \n",
    "                     names = ['Movie_Id', 'Year', 'Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Stand by Me 4.43\nFull Metal Jacket 4.43\nMemento 4.38\n"
     ]
    }
   ],
   "source": [
    "i = np.random.choice(list(top3_recommendations.keys()))\n",
    "\n",
    "print_recs(i)"
   ]
  },
  {
   "source": [
    "Давайте посмотрим, что смотрел этот человек, и выберем из нашего датасета те фильмы, которые этот человек оценил на 5."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "films = data.df[(data.df.Cust_Id == i) & (data.df.Rating == 5)]['Movie_Id'].values\n",
    "titles[titles.Movie_Id.isin(films)]['Name'].values"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 71,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['High Fidelity', 'American Beauty', 'Clerks',\n",
       "       \"Aliens: Collector's Edition\", \"Alien: Collector's Edition\",\n",
       "       'Lord of the Rings: The Fellowship of the Ring', 'Braveheart',\n",
       "       'Napoleon Dynamite', 'The Godfather', 'The Last Samurai',\n",
       "       'Batman Begins', 'The Sixth Sense',\n",
       "       'Dances With Wolves: Special Edition', 'Heat: Special Edition',\n",
       "       'True Romance', 'The Right Stuff', 'I',\n",
       "       'Star Wars: Episode V: The Empire Strikes Back',\n",
       "       'GoodFellas: Special Edition', 'The Sopranos: Season 3',\n",
       "       'Fight Club', 'Sin City', 'Donnie Brasco', 'Good Morning',\n",
       "       'Spanglish', 'Gremlins', 'Billy Madison', 'Armageddon',\n",
       "       'The Usual Suspects',\n",
       "       'Lord of the Rings: The Two Towers: Extended Edition',\n",
       "       'The Princess Bride',\n",
       "       'The Lord of the Rings: The Fellowship of the Ring: Extended Edition',\n",
       "       'Apollo 13', 'Willow', 'The Fog of War', 'Happy Gilmore',\n",
       "       'Good Will Hunting', 'The Goonies', 'The Natural',\n",
       "       'The Fifth Element', 'Die Hard', 'Raiders of the Lost Ark',\n",
       "       'Seinfeld: Season 4', 'Back to the Future', 'Pulp Fiction',\n",
       "       'Forrest Gump', 'Layer Cake', 'Lord of the Rings: The Two Towers',\n",
       "       'The Godfather', 'Swingers', 'Office Space',\n",
       "       'Lord of the Rings: The Return of the King',\n",
       "       'The Sopranos: Season 2', 'Spider-Man', '12 Monkeys',\n",
       "       'The Shawshank Redemption: Special Edition', 'The Matrix',\n",
       "       'Lord of the Rings: The Return of the King: Extended Edition',\n",
       "       'Leaving Las Vegas', 'Traffic', 'Seinfeld: Seasons 1 & 2',\n",
       "       'The Sopranos: Season 1', 'Star Wars: Episode IV: A New Hope',\n",
       "       'Old School', 'Seinfeld: Season 3', 'The Ring'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ]
  },
  {
   "source": [
    "# 9.9. Заключение"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 9.10. Задачи (7 задач)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}